# -*- coding: utf-8 -*-
import os
import sys
import json
import time
import openai
from openai import OpenAI

# è®¾ç½®UTF-8ç¼–ç è¾“å‡ºï¼Œè§£å†³Windowsæ§åˆ¶å°æ˜¾ç¤ºé—®é¢˜
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

class OpenAIProxyService:
    """
    ä½¿ç”¨æœ¬åœ°Geminiä»£ç†çš„OpenAIå®¢æˆ·ç«¯æœåŠ¡
    é€šè¿‡æœ¬åœ°ä»£ç†æœåŠ¡å™¨è®¿é—®Gemini APIï¼Œé¿å…ç›´æ¥ç½‘ç»œè¿æ¥é—®é¢˜
    """
    
    def __init__(self, proxy_url="http://localhost:8080", debug=False):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨æœ¬åœ°Geminiä»£ç†
        
        Args:
            proxy_url: æœ¬åœ°ä»£ç†æœåŠ¡å™¨åœ°å€
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        self.proxy_url = proxy_url
        self.debug = debug
        self.api_key = os.getenv('GEMINI_API_KEY')
        
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°GEMINI_API_KEYç¯å¢ƒå˜é‡")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ŒæŒ‡å‘æœ¬åœ°ä»£ç†
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=f"{self.proxy_url}/v1"
        )
        
                                if self.debug:
                print(f"âœ… OpenAIä»£ç†å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ", file=sys.stderr)
                print(f"ğŸ“ ä»£ç†åœ°å€: {self.proxy_url}", file=sys.stderr)
                print(f"ğŸ”‘ APIå¯†é’¥: {self.api_key[:10]}...{self.api_key[-4:]}", file=sys.stderr)

    def process_text(self, text, model="gpt-3.5-turbo", temperature=0.7, system_instruction=None):
        """
        å¤„ç†æ–‡æœ¬è¯·æ±‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            model: æ¨¡å‹åç§°ï¼ˆä½¿ç”¨OpenAIæ¨¡å‹åï¼Œä¼šè‡ªåŠ¨æ˜ å°„åˆ°Geminiï¼‰
            temperature: æ¸©åº¦å‚æ•°
            system_instruction: ç³»ç»ŸæŒ‡ä»¤
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            if self.debug:
                print(f"ğŸ”¤ å¤„ç†æ–‡æœ¬: {text[:50]}...")
                print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            
            if system_instruction:
                messages.append({"role": "system", "content": system_instruction})
            
            messages.append({"role": "user", "content": text})
            
            # å‘é€è¯·æ±‚
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=4000
            )
            
            result_text = response.choices[0].message.content
            
            if self.debug:
                print(f"âœ… æ–‡æœ¬å¤„ç†æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(result_text)}")
            
            return {
                'success': True,
                'text': result_text,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'completion_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                },
                'model': response.model,
                'finish_reason': response.choices[0].finish_reason
            }
            
        except Exception as e:
            if self.debug:
                print(f"âŒ æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'text': None
            }

    def process_video(self, video_path, prompt, model="gpt-4", temperature=0.7, system_instruction=None):
        """
        å¤„ç†è§†é¢‘æ–‡ä»¶ï¼ˆæ³¨æ„ï¼šå½“å‰é€šè¿‡ä»£ç†å¯èƒ½ä¸æ”¯æŒè§†é¢‘ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            prompt: åˆ†ææç¤º
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            system_instruction: ç³»ç»ŸæŒ‡ä»¤
            
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            if self.debug:
                print(f"ğŸ¥ å¤„ç†è§†é¢‘: {video_path}")
                print(f"ğŸ“ æç¤ºè¯: {prompt[:50]}...")
            
            # æ³¨æ„ï¼šOpenAIä»£ç†å¯èƒ½ä¸ç›´æ¥æ”¯æŒè§†é¢‘ä¸Šä¼ 
            # è¿™é‡Œå…ˆè¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„é”™è¯¯ï¼Œå»ºè®®å›é€€åˆ°åŸå§‹Gemini API
            return {
                'success': False,
                'error': 'Video processing not supported through proxy, use direct Gemini API',
                'text': None,
                'fallback_to_direct': True
            }
            
        except Exception as e:
            if self.debug:
                print(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'text': None
            }

    def get_embeddings(self, texts, model="text-embedding-ada-002"):
        """
        è·å–æ–‡æœ¬åµŒå…¥
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨æˆ–å•ä¸ªæ–‡æœ¬
            model: åµŒå…¥æ¨¡å‹åç§°
            
        Returns:
            dict: åµŒå…¥ç»“æœ
        """
        try:
            if self.debug:
                print(f"ğŸ“Š è·å–åµŒå…¥ï¼Œæ–‡æœ¬æ•°é‡: {len(texts) if isinstance(texts, list) else 1}")
            
            response = self.client.embeddings.create(
                model=model,
                input=texts
            )
            
            embeddings = [item.embedding for item in response.data]
            
            if self.debug:
                print(f"âœ… åµŒå…¥è·å–æˆåŠŸï¼Œç»´åº¦: {len(embeddings[0]) if embeddings else 0}")
            
            return {
                'success': True,
                'embeddings': embeddings,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'total_tokens': response.usage.total_tokens
                },
                'model': response.model
            }
            
        except Exception as e:
            if self.debug:
                print(f"âŒ åµŒå…¥è·å–å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'embeddings': None
            }

    def test_connection(self):
        """
        æµ‹è¯•ä»£ç†è¿æ¥
        
        Returns:
            dict: æµ‹è¯•ç»“æœ
        """
        try:
            if self.debug:
                print("ğŸ”§ æµ‹è¯•ä»£ç†è¿æ¥...")
            
            start_time = time.time()
            
            # å‘é€ç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": "è¯·å›å¤ï¼šè¿æ¥æµ‹è¯•æˆåŠŸ"}],
                max_tokens=50
            )
            
            end_time = time.time()
            response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            result_text = response.choices[0].message.content
            
            if self.debug:
                print(f"âœ… ä»£ç†è¿æ¥æµ‹è¯•æˆåŠŸ")
                print(f"â±ï¸  å“åº”æ—¶é—´: {response_time:.2f}ms")
                print(f"ğŸ“ å“åº”å†…å®¹: {result_text}")
            
            return {
                'success': True,
                'message': 'ä»£ç†è¿æ¥æ­£å¸¸',
                'response_text': result_text,
                'response_time': response_time,
                'proxy_url': self.proxy_url
            }
            
        except Exception as e:
            if self.debug:
                print(f"âŒ ä»£ç†è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': 'ä»£ç†è¿æ¥å¤±è´¥',
                'proxy_url': self.proxy_url
            }

def main():
    """
    å‘½ä»¤è¡Œå…¥å£å‡½æ•°
    """
    if len(sys.argv) < 3:
        print(json.dumps({
            'success': False,
            'error': 'Usage: python openai_proxy_service.py <type> <content> [options]'
        }))
        return
    
    # è§£æå‚æ•°
    action_type = sys.argv[1]
    content = sys.argv[2]
    
    # è§£æå¯é€‰å‚æ•°
    model = "gpt-3.5-turbo"
    temperature = 0.7
    system_instruction = None
    debug = os.getenv('DEBUG') == 'true'
    proxy_url = os.getenv('GEMINI_PROXY_URL', 'http://localhost:8080')
    
    for i in range(3, len(sys.argv)):
        arg = sys.argv[i]
        if arg.startswith('model='):
            model = arg.split('=', 1)[1]
        elif arg.startswith('temperature='):
            temperature = float(arg.split('=', 1)[1])
        elif arg.startswith('system='):
            system_instruction = arg.split('=', 1)[1]
        elif arg.startswith('proxy='):
            proxy_url = arg.split('=', 1)[1]
    
    try:
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        service = OpenAIProxyService(proxy_url=proxy_url, debug=debug)
        
        # æ ¹æ®æ“ä½œç±»å‹å¤„ç†
        if action_type == 'text':
            result = service.process_text(
                text=content,
                model=model,
                temperature=temperature,
                system_instruction=system_instruction
            )
        elif action_type == 'video':
            result = service.process_video(
                video_path=content,
                prompt=system_instruction or "åˆ†æè¿™ä¸ªè§†é¢‘å†…å®¹",
                model=model,
                temperature=temperature
            )
        elif action_type == 'embedding':
            result = service.get_embeddings(
                texts=content,
                model=model
            )
        elif action_type == 'test':
            result = service.test_connection()
        else:
            result = {
                'success': False,
                'error': f'Unknown action type: {action_type}'
            }
        
        # è¾“å‡ºç»“æœ
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        print(json.dumps({
            'success': False,
            'error': str(e),
            'type': 'initialization_error'
        }, ensure_ascii=False))

if __name__ == "__main__":
    main() 